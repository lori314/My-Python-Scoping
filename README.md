# Python网络爬虫学习日志 (My Python Web-Scraping Journey)

👋 **欢迎来到我的个人学习仓库！**

这个仓库是我从大一开始，自学Python网络爬虫技术至今的所有代码、笔记和项目的集合。它并非一个单一的、可执行的“产品”，而更像一本公开的**开发者日志**，记录了我是如何从零开始，一步步探索和实践网络数据采集技术的。

在这里，你将看到我学习的完整轨迹：从发送第一个HTTP请求，到解析复杂的HTML页面，再到使用自动化工具和异步框架，最后尝试将技术封装成一个可用的GUI应用。
---

## 🛠️ 技能汇总

*   **HTTP请求与会话:** `requests`
*   **数据解析技术:** `lxml (XPath)`, `Beautiful Soup`, `正则表达式 (Regex)`
*   **动态网页处理:** `Selenium`, `Pyppeteer`
*   **并发与异步:** `asyncio`, `多线程`
*   **爬虫框架:** `Scrapy`
*   **桌面应用开发:** `Tkinter`
*   **基础工具:** `Git`, `pip` & `requirements.txt`
---

## 🗺️ 学习路径导览 (项目结构)

我将所有的代码和笔记按照学习阶段和技术主题，整理成了以下几个模块。每个模块都代表了我学习过程中的一个里程碑。

### **1. `1_Requests_and_Parsing_Basics/` - 基础入门**

这是我学习的起点。在这个模块中，我主要练习了：

*   使用 `requests` 库发送基本的 `GET` 和 `POST` 请求。
*   学习如何构造请求头 (`Headers`) 和参数 (`params`/`data`) 来模拟浏览器行为。
*   初步尝试了三种核心的HTML解析技术：
    *   **XPath (`lxml`)**: 精准的路径选择。
    *   **BeautifulSoup**: 对新手友好的DOM遍历。
    *   **正则表达式 (Regex)**: 文本匹配的强大工具。

### **2. `2_Selenium_and_Dynamic_Pages/` - 应对JavaScript动态加载**

当遇到由JavaScript动态渲染内容的网站时，我学习并实践了浏览器自动化技术：

*   使用 `Selenium` 驱动真实的浏览器（如Edge/Chrome）。
*   通过代码实现模拟点击、输入、滚动等用户操作。
*   等待特定元素加载完成，以确保能抓取到动态生成的数据。
*   `Pyppeteer` 作为 `puppeteer` 的Python版本，也是我进行自动化探索的工具之一。

### **3. `3_Framework_and_Concurrency/` - 追求效率与规范**

为了提升爬取效率和代码的规范性，我开始探索更高级的主题：

*   **并发编程:** 通过 `asyncio` (协程) 和 `threading` (多线程) 的学习，我理解了如何同时执行多个网络任务，极大地缩短了等待时间。
*   **Scrapy框架:** 我学习了工业级的`Scrapy`框架，理解了其基于`Twisted`的异步IO核心，以及`Spider`, `Item`, `Pipeline`, `Middleware`等组件协同工作的流程。`firstblood` 文件夹就是一个标准的Scrapy项目模板。

### **4. `4_Mini_Projects_and_Applications/` - 综合应用与实践**

在这个模块，我将前面学到的多种技术综合起来，解决一些具体的、有趣的问题。每个子文件夹都是一个独立的小项目。

*   `01_ppt_downloader/`: 综合运用`requests`和`XPath`，实现对特定网站PPT模板的批量下载。
*   `02_resume_template_scraper.py`: 练习对多层页面进行跳转和抓取。
*   `03_pcgames_torrent_finder/`: 一个更复杂的爬虫，结合了`requests`和`Selenium`来处理需要交互才能获取下载链接的场景。
*   **`04_GUI_API_Tool_Demo/`**: **(✨本作品集核心亮点)**，详见下文介绍。

### **5. `5_Utilities_and_Experiments/` - 工具与实验**

这里存放了一些辅助脚本和我的技术好奇心探索。

*   `01_code_collector.py`: 一个我为方便整理代码而编写的Python脚本，能自动将项目中的所有`.py`文件整合成一个文本文件。
*   `02_hash_bruteforce_demo.py`: 出于对信息安全的好奇，我写了这个脚本来学习和演示哈希算法（如SHA256）的基本原理和暴力破解的可能性。这是一个纯粹的算法学习和安全意识教育的实践。

---

## ✨ 核心亮点项目：一个带GUI的API分析工具 (`4_GUI_API_Tool_Demo/`)

在所有这些学习和实践中，这个项目最能代表我的综合能力。

#### 背景

在学习了基础的网络请求后，我想挑战一个更复杂的场景：如何用Python代码去复现一个现代Web应用（如在线网盘、SaaS工具等）与后端服务器之间的完整数据交互流程，并将其封装成一个普通用户也能使用的桌面工具。这个项目就是我对这一课题的深入实践。

#### 我在这个项目中解决的技术挑战

*   **复杂的API请求模拟:**
    *   通过学习浏览器开发者工具，我分析了目标API的请求头(Headers)和请求体(Payload)结构。
    *   代码中成功构造了包含动态参数、认证Token和安全校验字段的复杂`POST`请求。

*   **从零构建GUI界面:**
    *   我使用Python的内置库`Tkinter`搭建了完整的图形用户界面。
    *   实现了输入、按钮点击、列表展示、滚动日志等多种交互组件，将命令行脚本变成了一个可用的产品原型。

*   **使用多线程解决UI卡顿:**
    *   为了解决进行网络请求时GUI界面会冻结的问题，我学习并应用了`threading`和`queue`模块。
    *   成功将所有耗时的网络操作都放入了后台子线程，并通过队列(Queue)与UI主线程进行安全的数据通信，保证了应用的流畅性。

这个项目让我深刻理解到，编写一个用户可用的软件，需要综合考虑功能实现、用户体验、程序健壮性等多个方面，极大地锻炼了我的**工程化能力**。

---

## 🚀 如何开始

1.  克隆本仓库到本地：
    ```bash
    git clone [你的仓库SSH或HTTPS链接]
    ```
2.  进入项目目录：
    ```bash
    cd My-Python-Scoping-Journey
    ```
3.  安装所有依赖库：
    ```bash
    pip install -r requirements.txt
    ```
4.  现在，你可以进入任意子目录，自由地探索和运行其中的`.py`脚本了。

---

感谢你的阅读！这个仓库是我技术成长的真实记录，我会持续更新和完善它。
